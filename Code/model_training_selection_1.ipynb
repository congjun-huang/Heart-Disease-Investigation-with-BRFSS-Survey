{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2635cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf3597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and test data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_train.index = X_train['Unnamed: 0']\n",
    "X_train = X_train.drop(['Unnamed: 0'], axis=1)\n",
    "X_train.index.name = ''\n",
    "\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_train.index = y_train['Unnamed: 0']\n",
    "y_train = y_train.drop(['Unnamed: 0'], axis=1)\n",
    "y_train.index.name = ''\n",
    "\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "X_test.index = X_test['Unnamed: 0']\n",
    "X_test = X_test.drop(['Unnamed: 0'], axis=1)\n",
    "X_test.index.name = ''\n",
    "\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "y_test.index = y_test['Unnamed: 0']\n",
    "y_test = y_test.drop(['Unnamed: 0'], axis=1)\n",
    "y_test.index.name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264164e",
   "metadata": {},
   "source": [
    "I selected 5 models including: Random Forest, Gradient Boosting, Bagging with XGBoost, Bagging with Decision Tree and Adaptive Boosting with Decision Tree. Then I utilized grid search with StratifiedKFold cross validation to tune model and find the best hyperparameters. I recorded best hyperparameters and the corresponding score (AUC of the ROC curve) for each model type to compare in the next model selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81479d3e",
   "metadata": {},
   "source": [
    "## Model 1: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a7f2e",
   "metadata": {},
   "source": [
    "First, we used Random Forest. This is a tree-based ensemble model, and could avoid the results that have low bias but high variance. However, Random Forest requires a large number of predictors to choose from at each split, and also it would incur heavy computation. The best Random Forest model is `RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=70)` and the corresponding AUC is 0.836."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15429c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "# perform grid search with Random Forest\n",
    "pg_rf = {'n_estimators': [50,60,70,80,90],\n",
    "         'criterion': ['gini','entropy'],\n",
    "         'max_depth':[10,20,30,40,50]}\n",
    "model_rf = GridSearchCV(\n",
    "           RandomForestClassifier(), \n",
    "           pg_rf, \n",
    "           cv=StratifiedKFold(3, shuffle=True, random_state=1234), \n",
    "           scoring='roc_auc', \n",
    "           verbose=5,\n",
    "           n_jobs=-1).fit(\n",
    "           X_train,y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1518e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=70)\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 70}\n",
      "0.8360940603417045\n"
     ]
    }
   ],
   "source": [
    "# record best estimator and best score\n",
    "print(model_rf.best_estimator_)\n",
    "print(model_rf.best_params_)\n",
    "print(model_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b5f85",
   "metadata": {},
   "source": [
    "## Model 2: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c1a53",
   "metadata": {},
   "source": [
    "Second, I used Gradient Boosting. This is a very robust algorithm combining Gradient descent and Boosting. The word ‘gradient’ implies that we can have two or more derivatives of the same function. Gradient Boosting has three main components: additive model, loss function and a weak learner. The best Gradient Boosting model is `GradientBoostingClassifier(max_depth=5, n_estimators=100)` and the corresponding AUC is 0.838."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c89b063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "# perform grid search with Gradient Boosting\n",
    "pg_gb = {'n_estimators': [100,300,500,800,1200],\n",
    "         'max_depth': [None,5,10,15]}\n",
    "model_gb = GridSearchCV(\n",
    "           GradientBoostingClassifier(), \n",
    "           pg_gb, \n",
    "           cv=StratifiedKFold(3, shuffle=True, random_state=1234), \n",
    "           scoring='roc_auc', \n",
    "           verbose=5,\n",
    "           n_jobs=-1).fit(\n",
    "           X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8825b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=5)\n",
      "{'max_depth': 5, 'n_estimators': 100}\n",
      "0.8376538895981197\n"
     ]
    }
   ],
   "source": [
    "# record best estimator and best score\n",
    "print(model_gb.best_estimator_)\n",
    "print(model_gb.best_params_)\n",
    "print(model_gb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6e097",
   "metadata": {},
   "source": [
    "## Model 3: Bagging with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc116c8a",
   "metadata": {},
   "source": [
    "Next, I used Bagging method with the XGBoost classifier as the base estimator. This is an alternative of Random Forest with a lighter computational pressure. A bagged tree is constructed in a similar fashion to Random Forest. The primary difference is that in a bagged model, all attributes are evaluated at each split in each tree. The best Bagging with XGBoost model is `BaggingClassifier(base_estimator=XGBClassifier(eval_metric='mlogloss', gamma=0, max_depth=5, min_child_weight=3), max_features=9, n_estimators=100)` and the corresponding AUC is 0.830."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "190a6c34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    }
   ],
   "source": [
    "# perform grid search with Bagging with XGBoost\n",
    "pg_xgb = {'n_estimators': [100,300,500],\n",
    "          'max_features': [1,5,7,9]}\n",
    "model_xgb = GridSearchCV(\n",
    "            BaggingClassifier(base_estimator=XGBClassifier(eval_metric='mlogloss',\n",
    "                                                           gamma=0,\n",
    "                                                           max_depth=5,\n",
    "                                                           min_child_weight=3),\n",
    "                                                           random_state=1234),\n",
    "            pg_xgb, \n",
    "            cv=StratifiedKFold(3, shuffle=True, random_state=1234), \n",
    "            scoring='roc_auc', \n",
    "            verbose=5,\n",
    "            n_jobs=-1).fit(\n",
    "            X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54bfb7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                               colsample_bylevel=None,\n",
      "                                               colsample_bynode=None,\n",
      "                                               colsample_bytree=None,\n",
      "                                               enable_categorical=False,\n",
      "                                               eval_metric='mlogloss', gamma=0,\n",
      "                                               gpu_id=None,\n",
      "                                               importance_type=None,\n",
      "                                               interaction_constraints=None,\n",
      "                                               learning_rate=None,\n",
      "                                               max_delta_step=None, max_depth=5,\n",
      "                                               min_child_weight=3, missing=nan,\n",
      "                                               monotone_constraints=None,\n",
      "                                               n_estimators=100, n_jobs=None,\n",
      "                                               num_parallel_tree=None,\n",
      "                                               predictor=None,\n",
      "                                               random_state=None,\n",
      "                                               reg_alpha=None, reg_lambda=None,\n",
      "                                               scale_pos_weight=None,\n",
      "                                               subsample=None, tree_method=None,\n",
      "                                               validate_parameters=None,\n",
      "                                               verbosity=None),\n",
      "                  max_features=9, n_estimators=100, random_state=1234)\n",
      "{'max_features': 9, 'n_estimators': 100}\n",
      "0.8299052138941421\n"
     ]
    }
   ],
   "source": [
    "# record best estimator and best score\n",
    "print(model_xgb.best_estimator_)\n",
    "print(model_xgb.best_params_)\n",
    "print(model_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d878355",
   "metadata": {},
   "source": [
    "## Model 4: Bagging with Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167baf76",
   "metadata": {},
   "source": [
    "After that, I utilized the Bagging classifier with classic Decision Tree model as the base estimator. This is a reasonable choice for our binary classification problem since the interpretation of a tree model is relatively transparent and straightforward. The best Bagging with Decision Tree model is `BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), max_features=15, n_estimators=1200)` and the corresponding AUC is 0.834."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6b5854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 63 candidates, totalling 189 fits\n"
     ]
    }
   ],
   "source": [
    "# perform grid search with Bagging with Decision Tree\n",
    "pg_dt = {'n_estimators':[100,120,200,300,500,800,1200],\n",
    "         'max_features': [1,3,5,7,9,12,15,17,25]}\n",
    "model_dt = GridSearchCV(\n",
    "           BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),random_state=1234), \n",
    "           pg_dt, \n",
    "           cv=StratifiedKFold(3, shuffle=True, random_state=1234), \n",
    "           scoring='roc_auc', \n",
    "           verbose=5,\n",
    "           n_jobs=-1).fit(\n",
    "           X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99680bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),\n",
      "                  max_features=15, n_estimators=1200, random_state=1234)\n",
      "{'max_features': 15, 'n_estimators': 1200}\n",
      "0.8337653522569836\n"
     ]
    }
   ],
   "source": [
    "# record best estimator and best score\n",
    "print(model_dt.best_estimator_)\n",
    "print(model_dt.best_params_)\n",
    "print(model_dt.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f320f",
   "metadata": {},
   "source": [
    "## Model 5: Adaptive Boosting with Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015c7e1",
   "metadata": {},
   "source": [
    "Finally, I used Adaptive Boosting with Decision Tree as the base estimator. This ensemble method automatically adjusts its parameters to the data based on the actual performance in the current iteration. Both the weights for re-weighting the data and the weights for the final aggregation are re-computed iteratively. This method usually leads to an improved performance compared to the classification by one tree or other single base-learner. The best Adaptive Boosting with Decision Tree model is `AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),learning_rate=0.01, n_estimators=300)` and the corresponding AUC is 0.837."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52552835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 35 candidates, totalling 105 fits\n"
     ]
    }
   ],
   "source": [
    "# perform grid search with Adaptive Boosting with Decision Tree\n",
    "pg_ada = {'n_estimators': [100,120,200,300,500,800,1200],\n",
    "          'learning_rate': [1e-4,1e-2,1e0,1e1,1e2]}\n",
    "model_ada = GridSearchCV(\n",
    "            AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),random_state=1234), \n",
    "            pg_ada, \n",
    "            cv=StratifiedKFold(3, shuffle=True, random_state=1234), \n",
    "            scoring='roc_auc', \n",
    "            verbose=5,\n",
    "            n_jobs=-1).fit(\n",
    "            X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778dc7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),\n",
      "                   learning_rate=0.01, n_estimators=300, random_state=1234)\n",
      "{'learning_rate': 0.01, 'n_estimators': 300}\n",
      "0.8368010736998991\n"
     ]
    }
   ],
   "source": [
    "# record best estimator and best score\n",
    "print(model_ada.best_estimator_)\n",
    "print(model_ada.best_params_)\n",
    "print(model_ada.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b4f93",
   "metadata": {},
   "source": [
    "I created a dataframe and compared AUC of each model type from `StratifiedKFold` cross validation in the above process. The model performance result is: Gradient Boosting (0.8377) > Adaptive Boosting with Decision Tree (0.8368) > Random Forest (0.8361) > Bagging with Decision Tree (0.8338) > Bagging with XGBoost (0.8299). However, the ranking based on best score from cross validation might change slightly in each running of the code. Therefore, I pickled all 5 tuned models out and compared them on the same validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8298853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Score (AUC)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.836094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.837654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging with XGBoost</th>\n",
       "      <td>0.829905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging with Decision Tree</th>\n",
       "      <td>0.833765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adaptive Boosting with Decision Tree</th>\n",
       "      <td>0.836801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Best Score (AUC)\n",
       "Random Forest                                 0.836094\n",
       "Gradient Boosting                             0.837654\n",
       "Bagging with XGBoost                          0.829905\n",
       "Bagging with Decision Tree                    0.833765\n",
       "Adaptive Boosting with Decision Tree          0.836801"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare best score\n",
    "data = [[model_rf.best_score_],[model_gb.best_score_],[model_xgb.best_score_], \n",
    "        [model_dt.best_score_],[model_ada.best_score_]]\n",
    "df_compare = pd.DataFrame(data, columns=['Best Score (AUC)'],\n",
    "                          index=['Random Forest','Gradient Boosting','Bagging with XGBoost',\n",
    "                                 'Bagging with Decision Tree','Adaptive Boosting with Decision Tree'])\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0c498a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle tuned models out\n",
    "with open('model_rf.pickle', 'wb') as file:\n",
    "    pickle.dump(model_rf.best_estimator_, file)\n",
    "with open('model_gb.pickle', 'wb') as file:\n",
    "    pickle.dump(model_gb.best_estimator_, file)\n",
    "with open('model_xgb.pickle', 'wb') as file:\n",
    "    pickle.dump(model_xgb.best_estimator_, file)\n",
    "with open('model_dt.pickle', 'wb') as file:\n",
    "    pickle.dump(model_dt.best_estimator_, file)\n",
    "with open('model_ada.pickle', 'wb') as file:\n",
    "    pickle.dump(model_ada.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149c269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
